{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Monet Painting Generator using CycleGAN and PyTorch","metadata":{}},{"cell_type":"markdown","source":"This project aims to implement the CycleGAN archicture for the task of translating regular photos to Monet-style paintings. The architectures implemented from the paper Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (https://arxiv.org/abs/1703.10593).","metadata":{}},{"cell_type":"markdown","source":"## Setup and Helper Functions","metadata":{}},{"cell_type":"code","source":"import os\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport itertools\n\nimport torch\nimport glob\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-25T17:34:55.251861Z","iopub.execute_input":"2023-07-25T17:34:55.252222Z","iopub.status.idle":"2023-07-25T17:34:58.990334Z","shell.execute_reply.started":"2023-07-25T17:34:55.252191Z","shell.execute_reply":"2023-07-25T17:34:58.989327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_RGB(image):\n    rgb_image = Image.new(\"RGB\", image.size)\n    rgb_image.paste(image)\n    return rgb_image","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:35:01.167589Z","iopub.execute_input":"2023-07-25T17:35:01.168180Z","iopub.status.idle":"2023-07-25T17:35:01.186535Z","shell.execute_reply.started":"2023-07-25T17:35:01.168148Z","shell.execute_reply":"2023-07-25T17:35:01.185456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following code implements the replay buffer, which is used by the paper to improve the training stability of the discriminator","metadata":{}},{"cell_type":"code","source":"class ReplayBuffer:\n    # Create image buffer to store previous 50 images\n    def __init__(self, max_size=50):\n        self.max_size = max_size\n        self.data = []\n\n    def push_and_pop(self, data):\n        to_return = []\n        for element in data.data:\n            element = torch.unsqueeze(element, 0)\n            if len(self.data) < self.max_size:\n                self.data.append(element)\n                to_return.append(element)\n            else:\n                if random.uniform(0, 1) > 0.5:\n                    i = random.randint(0, self.max_size - 1)\n                    to_return.append(self.data[i].clone())\n                    self.data[i] = element\n                else:\n                    to_return.append(element)\n        return Variable(torch.cat(to_return))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This initializes the weights of the model according to the paper","metadata":{}},{"cell_type":"code","source":"def initialize_conv_weights_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n        if hasattr(m, \"bias\") and m.bias is not None:\n            torch.nn.init.constant_(m.bias.data, 0.0)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Class for our custom dataset","metadata":{}},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n\n        if mode == 'train':\n            self.monet_files = sorted(glob.glob(os.path.join(root, \"monet_jpg\") + \"/*.*\")[:600])\n            self.photo_files = sorted(glob.glob(os.path.join(root, \"photo_jpg\") + \"/*.*\")[:250])\n        elif mode == 'test':\n            self.monet_files = sorted(glob.glob(os.path.join(root, \"monet_jpg\") + \"/*.*\")[250:])\n            self.photo_files = sorted(glob.glob(os.path.join(root, \"photo_jpg\") + \"/*.*\")[250:300])\n        elif mode == 'all':\n            self.monet_files = sorted(glob.glob(os.path.join(root, \"monet_jpg\") + \"/*.*\"))\n            self.photo_files = sorted(glob.glob(os.path.join(root, \"photo_jpg\") + \"/*.*\"))\n\n    def __getitem__(self, index):\n        monet = Image.open(self.monet_files[index % len(self.monet_files)])\n        photo = Image.open(self.photo_files[random.randint(0, len(self.photo_files) - 1)])\n\n        if monet.mode != \"RGB\":\n            monet = convert_to_RGB(monet)\n        if photo.mode != \"RGB\":\n            photo = convert_to_RGB(photo)\n\n        monet = self.transform(monet)\n        photo = self.transform(photo)\n\n        return (monet.float(), photo.float())\n\n    def __len__(self):\n        return max(len(self.monet_files), len(self.photo_files))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the Network","metadata":{}},{"cell_type":"code","source":"# Residual block with two convolution layers\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channel):\n        super(ResidualBlock, self).__init__()\n\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_channel, in_channel, 3),\n            nn.InstanceNorm2d(in_channel),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_channel, in_channel, 3),\n            nn.InstanceNorm2d(in_channel),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\n\n# Generator from CycleGAN paper\n# c7s1-64,d128,d256,R256,R256,R256, R256,R256,R256,R256,R256,R256,u128 u64,c7s1-3\nclass GeneratorResNet(nn.Module):\n    def __init__(self, input_shape, num_residual_blocks):\n        super(GeneratorResNet, self).__init__()\n\n        channels = input_shape[0]\n\n        # Initial convolution block\n        out_channels = 64\n        model = [\n            nn.ReflectionPad2d(channels),\n            nn.Conv2d(channels, out_channels, kernel_size = 7),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        ]\n        in_channels = out_channels\n\n        # Downsampling (Encoder)\n        for _ in range(2):\n            out_channels *= 2\n            model += [\n                nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 2, padding = 1),\n                nn.InstanceNorm2d(out_channels),\n                nn.ReLU(inplace=True),\n            ]\n            in_channels = out_channels\n\n        # Residual blocks\n        for _ in range(num_residual_blocks):\n            model += [ResidualBlock(out_channels)]\n\n        # Upsampling (Decoder)\n        for _ in range(2):\n            out_channels //= 2\n            model += [\n                nn.Upsample(scale_factor = 2),\n                nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n                nn.InstanceNorm2d(out_channels),\n                nn.ReLU(inplace=True),\n            ]\n            in_channels = out_channels\n\n        # Output layer\n        model += [\n            nn.ReflectionPad2d(channels),\n            nn.Conv2d(out_channels, channels, 7),\n            nn.Tanh(),\n        ]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)\n\n\n# Discriminator from CycleGAN paper\n# C64-C128-C256-C512\nclass Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        channels, height, width = input_shape\n\n        # Calculate output shape of discriminator\n        self.output_shape = (1, height //2 ** 4, width // 2 ** 4)\n\n        def discriminator_block(in_channels, out_channels, normalize=True):\n            layers = [\n                nn.Conv2d(in_channels, out_channels, kernel_size = 4, stride = 2, padding = 1)\n            ]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_channels))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, out_channels=64, normalize=False),\n            *discriminator_block(64, out_channels=128),\n            *discriminator_block(128, out_channels=256),\n            *discriminator_block(256, out_channels=512),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, padding=1)\n        )\n\n    def forward(self, img):\n        return self.model(img)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:35:04.337857Z","iopub.execute_input":"2023-07-25T17:35:04.338230Z","iopub.status.idle":"2023-07-25T17:35:04.357177Z","shell.execute_reply.started":"2023-07-25T17:35:04.338198Z","shell.execute_reply":"2023-07-25T17:35:04.355981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"Training function","metadata":{}},{"cell_type":"code","source":"def train(gen_G, gen_F, dX, dY, dataloader, n_epochs, id_loss, cycle_loss, gan_loss, \n          lambda_cycle, lambda_id, optimizer_G, optimizer_dX, optimizer_dY, buffer_X, buffer_Y, device):\n    for epoch in range(n_epochs):\n        for i, (monet, photo) in enumerate(dataloader):\n            monet = monet.to(device)\n            photo = photo.to(device)\n\n            valid = torch.from_numpy(np.ones((monet.size(0), *dX.output_shape), dtype=\"float32\")).to(device)\n            generated = torch.from_numpy(np.zeros((monet.size(0), *dX.output_shape), dtype=\"float32\")).to(device)\n\n            # TRAIN GENERATORS\n            gen_G.train()\n            gen_F.train()\n            optimizer_G.zero_grad()\n            \n            # Identity Loss\n            id_loss_G = id_loss(gen_G(photo), photo)\n            id_loss_F = id_loss(gen_F(monet), monet)\n            id_loss_avg = (id_loss_G + id_loss_F)/2\n            \n            # GAN Loss\n            generated_monet = gen_G(photo)\n            generated_photo = gen_F(monet)\n            gan_loss_G = gan_loss(dY(generated_monet), valid)\n            gan_loss_F = gan_loss(dX(generated_photo), valid)\n            gan_loss_avg = (gan_loss_G + gan_loss_F)/2\n            \n            # Cycle Consistency Loss\n            cycle_loss_G = cycle_loss(gen_F(generated_monet), photo)\n            cycle_loss_F = cycle_loss(gen_G(generated_photo), monet)\n            cycle_loss_avg = (cycle_loss_G + cycle_loss_F)/2\n\n            generator_loss = gan_loss_avg + lambda_id * id_loss_avg + lambda_cycle * cycle_loss_avg\n            generator_loss.backward()\n            optimizer_G.step()\n\n            # TRAIN DISCRIMINATOR X\n            optimizer_dX.zero_grad()\n            real_loss = gan_loss(dX(photo), valid)\n            generated_photo_ = buffer_X.push_and_pop(generated_photo)\n            generated_loss = gan_loss(dX(generated_photo_), generated)\n            dX_loss = (real_loss + generated_loss)/2\n            dX_loss.backward()\n            optimizer_dX.step()\n\n            # TRAIN DISCRIMINATOR Y\n            optimizer_dY.zero_grad()\n            real_loss = gan_loss(dY(monet), valid)\n            generated_monet_ = buffer_Y.push_and_pop(generated_monet)\n            generated_loss = gan_loss(dY(generated_monet_), generated)\n            dY_loss = (real_loss + generated_loss)/2\n            dY_loss.backward()\n            optimizer_dY.step()\n\n            d_loss = (dX_loss + dY_loss)/2\n            \n            if (i + 1) % 15 == 0:\n                print(f'[Epoch {epoch}/{n_epochs}] [Batch {i}/{len(dataloader)}] [Discriminator Loss {d_loss.item()}] [Generator Loss {generator_loss.item()}]')\n            \"\"\"\n            if (epoch + 1) % 5 == 1 and i == 62:\n                generated_monet = generated_monet/2 + 0.5\n                photo = photo/2 + 0.5\n                generated_monet = np.transpose(generated_monet.detach().cpu().numpy()[0, :, :, :])\n                photo = np.transpose(photo.detach().cpu().numpy()[0, :, :, :])\n                plt.imshow(generated_monet)\n                plt.show()\n                plt.imshow(photo)\n                plt.show()\n            \"\"\"\n        \"\"\"\n        torch.save(gen_G.state_dict(), \"generator_G\")\n        torch.save(gen_F.state_dict(), \"generator_F\")\n        \"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:35:07.493508Z","iopub.execute_input":"2023-07-25T17:35:07.493890Z","iopub.status.idle":"2023-07-25T17:35:07.509012Z","shell.execute_reply.started":"2023-07-25T17:35:07.493857Z","shell.execute_reply":"2023-07-25T17:35:07.507855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Parameters for training","metadata":{}},{"cell_type":"code","source":"params = {\n    \"n_epochs\": 150,\n    \"batch_size\": 4,\n    \"lr\": 0.0002,\n    \"b1\": 0.5,\n    \"b2\": 0.999,\n    \"img_size\": 256,\n    \"channels\": 3,\n    \"num_residual_blocks\": 12,\n    \"lambda_cycle\": 10.0,\n    \"lambda_id\": 5.0\n}","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:35:10.090080Z","iopub.execute_input":"2023-07-25T17:35:10.090440Z","iopub.status.idle":"2023-07-25T17:35:10.095902Z","shell.execute_reply.started":"2023-07-25T17:35:10.090411Z","shell.execute_reply":"2023-07-25T17:35:10.094553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Implement the dataloader","metadata":{}},{"cell_type":"code","source":"root = \"/kaggle/input/gan-getting-started\"\n\ntransforms_ = [\n    transforms.Resize((params['img_size'], params['img_size']), Image.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\n\ntrain_dataloader = DataLoader(\n    ImageDataset(root, mode=\"train\", transforms_=transforms_),\n    batch_size=params['batch_size'],\n    shuffle=True,\n    num_workers=1,\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:35:11.671274Z","iopub.execute_input":"2023-07-25T17:35:11.671629Z","iopub.status.idle":"2023-07-25T17:35:12.035690Z","shell.execute_reply.started":"2023-07-25T17:35:11.671600Z","shell.execute_reply":"2023-07-25T17:35:12.034711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Implement other components necessary for training, including the loss functions, models, buffers, and optimizers","metadata":{}},{"cell_type":"code","source":"gan_loss = torch.nn.MSELoss()\ncycle_loss = torch.nn.L1Loss()\nid_loss = torch.nn.L1Loss()\ninput_shape = (params['channels'], params['img_size'], params['img_size'])\n\ngen_G = GeneratorResNet(input_shape, params['num_residual_blocks'])\ngen_F = GeneratorResNet(input_shape, params['num_residual_blocks'])\ndX = Discriminator(input_shape) \ndY = Discriminator(input_shape)\n\ngen_G = gen_G.to(device)\ngen_F = gen_F.to(device)\ndX = dX.to(device)\ndY = dY.to(device)\n\ngen_G.apply(initialize_conv_weights_normal)\ngen_F.apply(initialize_conv_weights_normal)\ndX.apply(initialize_conv_weights_normal)\ndY.apply(initialize_conv_weights_normal)\n\nbuffer_X = ReplayBuffer()\nbuffer_Y = ReplayBuffer()\n\noptimizer_G = torch.optim.Adam(\n    itertools.chain(gen_G.parameters(), gen_F.parameters()),\n    lr=params['lr'],\n    betas=(params['b1'], params['b2']),\n)\noptimizer_dX = torch.optim.Adam(dX.parameters(), lr=params['lr'], betas=(params['b1'], params['b2']))\noptimizer_dY = torch.optim.Adam(dY.parameters(), lr=params['lr'], betas=(params['b1'], params['b2']))","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:35:14.171902Z","iopub.execute_input":"2023-07-25T17:35:14.172261Z","iopub.status.idle":"2023-07-25T17:35:17.554958Z","shell.execute_reply.started":"2023-07-25T17:35:14.172231Z","shell.execute_reply":"2023-07-25T17:35:17.553982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Traing the network!","metadata":{}},{"cell_type":"code","source":"train(gen_G, gen_F, dX, dY, train_dataloader, params['n_epochs'], id_loss, cycle_loss, gan_loss, params['lambda_cycle'], params['lambda_id'], optimizer_G, optimizer_dX, optimizer_dY, buffer_X, buffer_Y, device)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:35:19.644999Z","iopub.execute_input":"2023-07-25T17:35:19.645351Z","iopub.status.idle":"2023-07-25T17:50:57.944587Z","shell.execute_reply.started":"2023-07-25T17:35:19.645323Z","shell.execute_reply":"2023-07-25T17:50:57.943060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"import PIL\n! mkdir ../images","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:51:58.451835Z","iopub.execute_input":"2023-07-25T17:51:58.452240Z","iopub.status.idle":"2023-07-25T17:51:59.507684Z","shell.execute_reply.started":"2023-07-25T17:51:58.452208Z","shell.execute_reply":"2023-07-25T17:51:59.506384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_dataloader = DataLoader(ImageDataset(root, transforms_, \"all\"), batch_size=1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:52:56.560555Z","iopub.execute_input":"2023-07-25T17:52:56.561256Z","iopub.status.idle":"2023-07-25T17:52:56.597738Z","shell.execute_reply.started":"2023-07-25T17:52:56.561222Z","shell.execute_reply":"2023-07-25T17:52:56.596799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen_G.eval()\n\nfor i, (monet, photo) in enumerate(submit_dataloader):\n    outputs = gen_G(photo.to(device))\n    outputs = np.transpose(outputs.cpu().detach().numpy(), [0, 2, 3, 1])\n    outputs = outputs / 2 + 0.5\n    output = (outputs[0, :, :, :] * 255).astype(np.uint8)\n    im = Image.fromarray(output).convert('RGB')\n    im.save(f'../images/output_img_{i}.jpg')\n    if (i + 1) % 100 == 1:\n        print(f\"Progress: {i}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:56:07.207184Z","iopub.execute_input":"2023-07-25T17:56:07.207547Z","iopub.status.idle":"2023-07-25T17:58:56.151836Z","shell.execute_reply.started":"2023-07-25T17:56:07.207518Z","shell.execute_reply":"2023-07-25T17:58:56.150652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","metadata":{"execution":{"iopub.status.busy":"2023-07-25T18:02:40.647377Z","iopub.execute_input":"2023-07-25T18:02:40.647798Z","iopub.status.idle":"2023-07-25T18:02:45.025606Z","shell.execute_reply.started":"2023-07-25T18:02:40.647763Z","shell.execute_reply":"2023-07-25T18:02:45.024637Z"},"trusted":true},"execution_count":null,"outputs":[]}]}