{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Monet Painting Generator using CycleGAN and PyTorch"]},{"cell_type":"markdown","metadata":{},"source":["This project aims to implement the CycleGAN archicture for the task of translating regular photos to Monet-style paintings. The architectures implemented from the paper Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (https://arxiv.org/abs/1703.10593)."]},{"cell_type":"markdown","metadata":{},"source":["## Setup and Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-25T17:34:55.252222Z","iopub.status.busy":"2023-07-25T17:34:55.251861Z","iopub.status.idle":"2023-07-25T17:34:58.990334Z","shell.execute_reply":"2023-07-25T17:34:58.989327Z","shell.execute_reply.started":"2023-07-25T17:34:55.252191Z"},"trusted":true},"outputs":[],"source":["import os\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import itertools\n","\n","import torch\n","import glob\n","import random\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-25T17:35:01.168180Z","iopub.status.busy":"2023-07-25T17:35:01.167589Z","iopub.status.idle":"2023-07-25T17:35:01.186535Z","shell.execute_reply":"2023-07-25T17:35:01.185456Z","shell.execute_reply.started":"2023-07-25T17:35:01.168148Z"},"trusted":true},"outputs":[],"source":["def convert_to_RGB(image):\n","    rgb_image = Image.new(\"RGB\", image.size)\n","    rgb_image.paste(image)\n","    return rgb_image"]},{"cell_type":"markdown","metadata":{},"source":["The following code implements the replay buffer, which is used by the paper to improve the training stability of the discriminator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ReplayBuffer:\n","    # Create image buffer to store previous 50 images\n","    def __init__(self, max_size=50):\n","        self.max_size = max_size\n","        self.data = []\n","\n","    def push_and_pop(self, data):\n","        to_return = []\n","        for element in data.data:\n","            element = torch.unsqueeze(element, 0)\n","            if len(self.data) < self.max_size:\n","                self.data.append(element)\n","                to_return.append(element)\n","            else:\n","                if random.uniform(0, 1) > 0.5:\n","                    i = random.randint(0, self.max_size - 1)\n","                    to_return.append(self.data[i].clone())\n","                    self.data[i] = element\n","                else:\n","                    to_return.append(element)\n","        return Variable(torch.cat(to_return))"]},{"cell_type":"markdown","metadata":{},"source":["This initializes the weights of the model according to the paper"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def initialize_conv_weights_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","        if hasattr(m, \"bias\") and m.bias is not None:\n","            torch.nn.init.constant_(m.bias.data, 0.0)\n","    elif classname.find(\"BatchNorm2d\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant_(m.bias.data, 0.0)"]},{"cell_type":"markdown","metadata":{},"source":["Class for our custom dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ImageDataset(Dataset):\n","    def __init__(self, root, transforms_=None, mode=\"train\"):\n","        self.transform = transforms.Compose(transforms_)\n","\n","        if mode == 'train':\n","            self.monet_files = sorted(glob.glob(os.path.join(root, \"monet_jpg\") + \"/*.*\")[:600])\n","            self.photo_files = sorted(glob.glob(os.path.join(root, \"photo_jpg\") + \"/*.*\")[:250])\n","        elif mode == 'test':\n","            self.monet_files = sorted(glob.glob(os.path.join(root, \"monet_jpg\") + \"/*.*\")[250:])\n","            self.photo_files = sorted(glob.glob(os.path.join(root, \"photo_jpg\") + \"/*.*\")[250:300])\n","        elif mode == 'all':\n","            self.monet_files = sorted(glob.glob(os.path.join(root, \"monet_jpg\") + \"/*.*\"))\n","            self.photo_files = sorted(glob.glob(os.path.join(root, \"photo_jpg\") + \"/*.*\"))\n","\n","    def __getitem__(self, index):\n","        monet = Image.open(self.monet_files[index % len(self.monet_files)])\n","        photo = Image.open(self.photo_files[random.randint(0, len(self.photo_files) - 1)])\n","\n","        if monet.mode != \"RGB\":\n","            monet = convert_to_RGB(monet)\n","        if photo.mode != \"RGB\":\n","            photo = convert_to_RGB(photo)\n","\n","        monet = self.transform(monet)\n","        photo = self.transform(photo)\n","\n","        return (monet.float(), photo.float())\n","\n","    def __len__(self):\n","        return max(len(self.monet_files), len(self.photo_files))"]},{"cell_type":"markdown","metadata":{},"source":["## Building the Network"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-25T17:35:04.338230Z","iopub.status.busy":"2023-07-25T17:35:04.337857Z","iopub.status.idle":"2023-07-25T17:35:04.357177Z","shell.execute_reply":"2023-07-25T17:35:04.355981Z","shell.execute_reply.started":"2023-07-25T17:35:04.338198Z"},"trusted":true},"outputs":[],"source":["# Residual block with two convolution layers\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channel):\n","        super(ResidualBlock, self).__init__()\n","\n","        self.block = nn.Sequential(\n","            nn.ReflectionPad2d(1),\n","            nn.Conv2d(in_channel, in_channel, 3),\n","            nn.InstanceNorm2d(in_channel),\n","            nn.ReLU(inplace=True),\n","            nn.ReflectionPad2d(1),\n","            nn.Conv2d(in_channel, in_channel, 3),\n","            nn.InstanceNorm2d(in_channel),\n","        )\n","\n","    def forward(self, x):\n","        return x + self.block(x)\n","\n","\n","# Generator from CycleGAN paper\n","# c7s1-64,d128,d256,R256,R256,R256, R256,R256,R256,R256,R256,R256,u128 u64,c7s1-3\n","class GeneratorResNet(nn.Module):\n","    def __init__(self, input_shape, num_residual_blocks):\n","        super(GeneratorResNet, self).__init__()\n","\n","        channels = input_shape[0]\n","\n","        # Initial convolution block\n","        out_channels = 64\n","        model = [\n","            nn.ReflectionPad2d(channels),\n","            nn.Conv2d(channels, out_channels, kernel_size = 7),\n","            nn.InstanceNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        ]\n","        in_channels = out_channels\n","\n","        # Downsampling (Encoder)\n","        for _ in range(2):\n","            out_channels *= 2\n","            model += [\n","                nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 2, padding = 1),\n","                nn.InstanceNorm2d(out_channels),\n","                nn.ReLU(inplace=True),\n","            ]\n","            in_channels = out_channels\n","\n","        # Residual blocks\n","        for _ in range(num_residual_blocks):\n","            model += [ResidualBlock(out_channels)]\n","\n","        # Upsampling (Decoder)\n","        for _ in range(2):\n","            out_channels //= 2\n","            model += [\n","                nn.Upsample(scale_factor = 2),\n","                nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n","                nn.InstanceNorm2d(out_channels),\n","                nn.ReLU(inplace=True),\n","            ]\n","            in_channels = out_channels\n","\n","        # Output layer\n","        model += [\n","            nn.ReflectionPad2d(channels),\n","            nn.Conv2d(out_channels, channels, 7),\n","            nn.Tanh(),\n","        ]\n","\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","\n","# Discriminator from CycleGAN paper\n","# C64-C128-C256-C512\n","class Discriminator(nn.Module):\n","    def __init__(self, input_shape):\n","        super(Discriminator, self).__init__()\n","\n","        channels, height, width = input_shape\n","\n","        # Calculate output shape of discriminator\n","        self.output_shape = (1, height //2 ** 4, width // 2 ** 4)\n","\n","        def discriminator_block(in_channels, out_channels, normalize=True):\n","            layers = [\n","                nn.Conv2d(in_channels, out_channels, kernel_size = 4, stride = 2, padding = 1)\n","            ]\n","            if normalize:\n","                layers.append(nn.InstanceNorm2d(out_channels))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            *discriminator_block(channels, out_channels=64, normalize=False),\n","            *discriminator_block(64, out_channels=128),\n","            *discriminator_block(128, out_channels=256),\n","            *discriminator_block(256, out_channels=512),\n","            nn.ZeroPad2d((1, 0, 1, 0)),\n","            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, padding=1)\n","        )\n","\n","    def forward(self, img):\n","        return self.model(img)"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"markdown","metadata":{},"source":["Training function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-25T17:35:07.493890Z","iopub.status.busy":"2023-07-25T17:35:07.493508Z","iopub.status.idle":"2023-07-25T17:35:07.509012Z","shell.execute_reply":"2023-07-25T17:35:07.507855Z","shell.execute_reply.started":"2023-07-25T17:35:07.493857Z"},"trusted":true},"outputs":[],"source":["def train(gen_G, gen_F, dX, dY, dataloader, n_epochs, id_loss, cycle_loss, gan_loss, \n","          lambda_cycle, lambda_id, optimizer_G, optimizer_dX, optimizer_dY, buffer_X, buffer_Y, device):\n","    for epoch in range(n_epochs):\n","        for i, (monet, photo) in enumerate(dataloader):\n","            monet = monet.to(device)\n","            photo = photo.to(device)\n","\n","            valid = torch.from_numpy(np.ones((monet.size(0), *dX.output_shape), dtype=\"float32\")).to(device)\n","            generated = torch.from_numpy(np.zeros((monet.size(0), *dX.output_shape), dtype=\"float32\")).to(device)\n","\n","            # TRAIN GENERATORS\n","            gen_G.train()\n","            gen_F.train()\n","            optimizer_G.zero_grad()\n","            \n","            # Identity Loss\n","            id_loss_G = id_loss(gen_G(monet), monet)\n","            id_loss_F = id_loss(gen_F(photo), photo)\n","            id_loss_avg = (id_loss_G + id_loss_F)/2\n","            \n","            # GAN Loss\n","            generated_monet = gen_G(photo)\n","            generated_photo = gen_F(monet)\n","            gan_loss_G = gan_loss(dY(generated_monet), valid)\n","            gan_loss_F = gan_loss(dX(generated_photo), valid)\n","            gan_loss_avg = (gan_loss_G + gan_loss_F)/2\n","            \n","            # Cycle Consistency Loss\n","            cycle_loss_G = cycle_loss(gen_F(generated_monet), photo)\n","            cycle_loss_F = cycle_loss(gen_G(generated_photo), monet)\n","            cycle_loss_avg = (cycle_loss_G + cycle_loss_F)/2\n","\n","            generator_loss = gan_loss_avg + lambda_id * id_loss_avg + lambda_cycle * cycle_loss_avg\n","            generator_loss.backward()\n","            optimizer_G.step()\n","\n","            # TRAIN DISCRIMINATOR X\n","            optimizer_dX.zero_grad()\n","            real_loss = gan_loss(dX(photo), valid)\n","            generated_photo_ = buffer_X.push_and_pop(generated_photo)\n","            generated_loss = gan_loss(dX(generated_photo_), generated)\n","            dX_loss = (real_loss + generated_loss)/2\n","            dX_loss.backward()\n","            optimizer_dX.step()\n","\n","            # TRAIN DISCRIMINATOR Y\n","            optimizer_dY.zero_grad()\n","            real_loss = gan_loss(dY(monet), valid)\n","            generated_monet_ = buffer_Y.push_and_pop(generated_monet)\n","            generated_loss = gan_loss(dY(generated_monet_), generated)\n","            dY_loss = (real_loss + generated_loss)/2\n","            dY_loss.backward()\n","            optimizer_dY.step()\n","\n","            d_loss = (dX_loss + dY_loss)/2\n","            \n","            if (i + 1) % 15 == 0:\n","                print(f'[Epoch {epoch}/{n_epochs}] [Batch {i}/{len(dataloader)}] [Discriminator Loss {d_loss.item()}] [Generator Loss {generator_loss.item()}]')\n","            \"\"\"\n","            if (epoch + 1) % 5 == 1 and i == 62:\n","                generated_monet = generated_monet/2 + 0.5\n","                photo = photo/2 + 0.5\n","                generated_monet = np.transpose(generated_monet.detach().cpu().numpy()[0, :, :, :])\n","                photo = np.transpose(photo.detach().cpu().numpy()[0, :, :, :])\n","                plt.imshow(generated_monet)\n","                plt.show()\n","                plt.imshow(photo)\n","                plt.show()\n","            \"\"\"\n","        \"\"\"\n","        torch.save(gen_G.state_dict(), \"generator_G\")\n","        torch.save(gen_F.state_dict(), \"generator_F\")\n","        \"\"\""]},{"cell_type":"markdown","metadata":{},"source":["Parameters for training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-25T17:35:10.090440Z","iopub.status.busy":"2023-07-25T17:35:10.090080Z","iopub.status.idle":"2023-07-25T17:35:10.095902Z","shell.execute_reply":"2023-07-25T17:35:10.094553Z","shell.execute_reply.started":"2023-07-25T17:35:10.090411Z"},"trusted":true},"outputs":[],"source":["params = {\n","    \"n_epochs\": 150,\n","    \"batch_size\": 4,\n","    \"lr\": 0.0002,\n","    \"b1\": 0.5,\n","    \"b2\": 0.999,\n","    \"img_size\": 256,\n","    \"channels\": 3,\n","    \"num_residual_blocks\": 12,\n","    \"lambda_cycle\": 10.0,\n","    \"lambda_id\": 5.0\n","}"]},{"cell_type":"markdown","metadata":{},"source":["Implement the dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-25T17:35:11.671629Z","iopub.status.busy":"2023-07-25T17:35:11.671274Z","iopub.status.idle":"2023-07-25T17:35:12.035690Z","shell.execute_reply":"2023-07-25T17:35:12.034711Z","shell.execute_reply.started":"2023-07-25T17:35:11.671600Z"},"trusted":true},"outputs":[],"source":["root = \"/kaggle/input/gan-getting-started\"\n","\n","transforms_ = [\n","    transforms.Resize((params['img_size'], params['img_size']), Image.BICUBIC),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","]\n","\n","train_dataloader = DataLoader(\n","    ImageDataset(root, mode=\"train\", transforms_=transforms_),\n","    batch_size=params['batch_size'],\n","    shuffle=True,\n","    num_workers=1,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Implement other components necessary for training, including the loss functions, models, buffers, and optimizers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-25T17:35:14.172261Z","iopub.status.busy":"2023-07-25T17:35:14.171902Z","iopub.status.idle":"2023-07-25T17:35:17.554958Z","shell.execute_reply":"2023-07-25T17:35:17.553982Z","shell.execute_reply.started":"2023-07-25T17:35:14.172231Z"},"trusted":true},"outputs":[],"source":["gan_loss = torch.nn.MSELoss()\n","cycle_loss = torch.nn.L1Loss()\n","id_loss = torch.nn.L1Loss()\n","input_shape = (params['channels'], params['img_size'], params['img_size'])\n","\n","gen_G = GeneratorResNet(input_shape, params['num_residual_blocks'])\n","gen_F = GeneratorResNet(input_shape, params['num_residual_blocks'])\n","dX = Discriminator(input_shape) \n","dY = Discriminator(input_shape)\n","\n","gen_G = gen_G.to(device)\n","gen_F = gen_F.to(device)\n","dX = dX.to(device)\n","dY = dY.to(device)\n","\n","gen_G.apply(initialize_conv_weights_normal)\n","gen_F.apply(initialize_conv_weights_normal)\n","dX.apply(initialize_conv_weights_normal)\n","dY.apply(initialize_conv_weights_normal)\n","\n","buffer_X = ReplayBuffer()\n","buffer_Y = ReplayBuffer()\n","\n","optimizer_G = torch.optim.Adam(\n","    itertools.chain(gen_G.parameters(), gen_F.parameters()),\n","    lr=params['lr'],\n","    betas=(params['b1'], params['b2']),\n",")\n","optimizer_dX = torch.optim.Adam(dX.parameters(), lr=params['lr'], betas=(params['b1'], params['b2']))\n","optimizer_dY = torch.optim.Adam(dY.parameters(), lr=params['lr'], betas=(params['b1'], params['b2']))"]},{"cell_type":"markdown","metadata":{},"source":["Traing the network!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-25T17:35:19.645351Z","iopub.status.busy":"2023-07-25T17:35:19.644999Z","iopub.status.idle":"2023-07-25T17:50:57.944587Z","shell.execute_reply":"2023-07-25T17:50:57.943060Z","shell.execute_reply.started":"2023-07-25T17:35:19.645323Z"},"trusted":true},"outputs":[],"source":["train(gen_G, gen_F, dX, dY, train_dataloader, params['n_epochs'], id_loss, cycle_loss, gan_loss, params['lambda_cycle'], params['lambda_id'], optimizer_G, optimizer_dX, optimizer_dY, buffer_X, buffer_Y, device)"]},{"cell_type":"markdown","metadata":{},"source":["## Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-25T17:51:58.452240Z","iopub.status.busy":"2023-07-25T17:51:58.451835Z","iopub.status.idle":"2023-07-25T17:51:59.507684Z","shell.execute_reply":"2023-07-25T17:51:59.506384Z","shell.execute_reply.started":"2023-07-25T17:51:58.452208Z"},"trusted":true},"outputs":[],"source":["import PIL\n","! mkdir ../images"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-25T17:52:56.561256Z","iopub.status.busy":"2023-07-25T17:52:56.560555Z","iopub.status.idle":"2023-07-25T17:52:56.597738Z","shell.execute_reply":"2023-07-25T17:52:56.596799Z","shell.execute_reply.started":"2023-07-25T17:52:56.561222Z"},"trusted":true},"outputs":[],"source":["submit_dataloader = DataLoader(ImageDataset(root, transforms_, \"all\"), batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-25T17:56:07.207547Z","iopub.status.busy":"2023-07-25T17:56:07.207184Z","iopub.status.idle":"2023-07-25T17:58:56.151836Z","shell.execute_reply":"2023-07-25T17:58:56.150652Z","shell.execute_reply.started":"2023-07-25T17:56:07.207518Z"},"trusted":true},"outputs":[],"source":["gen_G.eval()\n","\n","for i, (monet, photo) in enumerate(submit_dataloader):\n","    outputs = gen_G(photo.to(device))\n","    outputs = np.transpose(outputs.cpu().detach().numpy(), [0, 2, 3, 1])\n","    outputs = outputs / 2 + 0.5\n","    output = (outputs[0, :, :, :] * 255).astype(np.uint8)\n","    im = Image.fromarray(output).convert('RGB')\n","    im.save(f'../images/output_img_{i}.jpg')\n","    if (i + 1) % 100 == 1:\n","        print(f\"Progress: {i}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-25T18:02:40.647798Z","iopub.status.busy":"2023-07-25T18:02:40.647377Z","iopub.status.idle":"2023-07-25T18:02:45.025606Z","shell.execute_reply":"2023-07-25T18:02:45.024637Z","shell.execute_reply.started":"2023-07-25T18:02:40.647763Z"},"trusted":true},"outputs":[],"source":["import shutil\n","shutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
